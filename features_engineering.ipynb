{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc8db0ec-c7fa-40a6-beb5-8748e682dbdc",
      "metadata": {
        "id": "dc8db0ec-c7fa-40a6-beb5-8748e682dbdc"
      },
      "source": [
        "# FEATURES ENGINEERING PIPELINE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "711be60c-5c47-4641-b394-74219e114caa",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "711be60c-5c47-4641-b394-74219e114caa"
      },
      "source": [
        "## Imports & Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8969f1dd-0f9e-427b-99c6-409153c397a5",
      "metadata": {
        "id": "8969f1dd-0f9e-427b-99c6-409153c397a5"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "10f3b374-fd19-4675-8c87-e22e6d52e554",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10f3b374-fd19-4675-8c87-e22e6d52e554",
        "outputId": "c32d49c4-9108-4f28-a9cd-1c2982006694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a385ff61-c2eb-46ec-bb2e-78543e2fea54",
      "metadata": {
        "id": "a385ff61-c2eb-46ec-bb2e-78543e2fea54"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Add the folder path where rouge is installed (change this to your actual path)\n",
        "sys.path.append(r\"C:\\Users\\Ait El Mouddene\\miniconda3\\envs\\py311-env\\Lib\\site-packages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2f862416-79ef-4150-ae7e-51faafe207c4",
      "metadata": {
        "id": "2f862416-79ef-4150-ae7e-51faafe207c4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8a6126ee-8f47-4247-b895-58daf5e6d319",
      "metadata": {
        "id": "8a6126ee-8f47-4247-b895-58daf5e6d319"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from rouge import Rouge\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1a284b3-d007-423a-93b9-8c13254925f4",
      "metadata": {
        "id": "b1a284b3-d007-423a-93b9-8c13254925f4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import lexnlp\n",
        "from lexnlp.extract.en.entities.nltk_re import get_companies\n",
        "from lexnlp.extract.en import conditions, acts, durations, dates, constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcf81f10-070d-49e3-98aa-c13ead03a398",
      "metadata": {
        "id": "dcf81f10-070d-49e3-98aa-c13ead03a398"
      },
      "outputs": [],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "911920c2-4f88-462b-a249-3b60973a437d",
      "metadata": {
        "id": "911920c2-4f88-462b-a249-3b60973a437d"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2237d328-05cf-4163-b804-e1ad8ca6d7c0",
      "metadata": {
        "id": "2237d328-05cf-4163-b804-e1ad8ca6d7c0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"/content/preprocessed_legal_data.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "YRvkdQjkrxyg"
      },
      "id": "YRvkdQjkrxyg"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_legal_terms(csv_path='legal_terms.csv'):\n",
        "    \"\"\"\n",
        "    Load legal terms from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        csv_path: Path to the CSV file containing legal terms\n",
        "\n",
        "    Returns:\n",
        "        A set of legal terms\n",
        "    \"\"\"\n",
        "    legal_terms = set()\n",
        "\n",
        "    with open(csv_path, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)  # Skip header row\n",
        "\n",
        "        for row in reader:\n",
        "            if len(row) >= 3:  # Check if the row has enough columns\n",
        "                term = row[2].lower()  # Term is in the third column\n",
        "                legal_terms.add(term)\n",
        "\n",
        "    print(f\"Loaded {len(legal_terms)} legal terms from {csv_path}\")\n",
        "    return legal_terms"
      ],
      "metadata": {
        "id": "rPUefTURr0MQ"
      },
      "id": "rPUefTURr0MQ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2911da1c-e84a-4d5d-bd21-ed445e9bc49c",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "2911da1c-e84a-4d5d-bd21-ed445e9bc49c"
      },
      "source": [
        "## Entities Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "31936b60-0659-4039-a9a5-a0314f57e1d5",
      "metadata": {
        "id": "31936b60-0659-4039-a9a5-a0314f57e1d5"
      },
      "outputs": [],
      "source": [
        "def calculate_entity_features(df):\n",
        "    \"\"\"\n",
        "    Extract entity-based features using LexNLP.\n",
        "\n",
        "    Returns:\n",
        "    - all_entity_features: list of dicts containing counts\n",
        "    - all_entity_features_text: list of dicts containing entity texts\n",
        "    \"\"\"\n",
        "    print(\"Calculating entity recognition features with LexNLP...\")\n",
        "\n",
        "    all_entity_features = []\n",
        "\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing documents for entity extraction\"):\n",
        "        doc_features_counts = []\n",
        "\n",
        "\n",
        "        for sentence in row['sentences']:\n",
        "            # Extract all entities\n",
        "            acts_ = list(acts.get_acts(sentence))\n",
        "            companies_ = list(get_companies(sentence))\n",
        "            conditions_ = list(conditions.get_conditions(sentence))\n",
        "            durations_ = list(durations.get_durations(sentence))\n",
        "            constraints_ = list(constraints.get_constraints(sentence))\n",
        "            dates_ = list(dates.get_dates(sentence))\n",
        "\n",
        "            # Store counts\n",
        "            count_features = {\n",
        "                'count_act_mention': len(acts_),\n",
        "                'count_company_mentions': len(companies_),\n",
        "                'count_conditions': len(conditions_),\n",
        "                'count_durations': len(durations_),\n",
        "                'count_constraints': len(constraints_),\n",
        "                'count_num_dates': len(dates_)\n",
        "            }\n",
        "\n",
        "            doc_features_counts.append(count_features)\n",
        "\n",
        "\n",
        "        all_entity_features.append(doc_features_counts)\n",
        "\n",
        "\n",
        "    return all_entity_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6965aa01-e357-49d5-9b7d-d8e0a882f77a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "6965aa01-e357-49d5-9b7d-d8e0a882f77a"
      },
      "source": [
        "## POS Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "78dc4a5d-c897-4d6b-a46c-144400dd250f",
      "metadata": {
        "id": "78dc4a5d-c897-4d6b-a46c-144400dd250f"
      },
      "outputs": [],
      "source": [
        "# Calculate POS tag features for each sentence\n",
        "def calculate_pos_features(df):\n",
        "    print(\"Calculating POS tag features...\")\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    # Count legal terms (basic implementation - could be expanded)\n",
        "    legal_terms = load_legal_terms()\n",
        "    all_pos_features = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing POS tags\"):\n",
        "        source_sentences = row['sentences']  # Use original sentences for POS analysis\n",
        "\n",
        "        doc_pos_features = []\n",
        "        for sentence in source_sentences:\n",
        "            # Skip empty sentences\n",
        "            if not sentence.strip():\n",
        "                doc_pos_features.append({\n",
        "                    'noun_ratio': 0.0,\n",
        "                    'verb_ratio': 0.0,\n",
        "                    'adj_ratio': 0.0,\n",
        "                    'adv_ratio': 0.0,\n",
        "                    'prop_noun_ratio': 0.0,\n",
        "                    'num_ratio': 0.0,\n",
        "                    'legal_term_count': 0\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            doc = nlp(sentence)\n",
        "\n",
        "            if len(doc) == 0:\n",
        "                doc_pos_features.append({\n",
        "                    'noun_ratio': 0.0,\n",
        "                    'verb_ratio': 0.0,\n",
        "                    'adj_ratio': 0.0,\n",
        "                    'adv_ratio': 0.0,\n",
        "                    'prop_noun_ratio': 0.0,\n",
        "                    'num_ratio': 0.0,\n",
        "                    'legal_term_count': 0\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Count POS tags\n",
        "            pos_counts = {}\n",
        "            for token in doc:\n",
        "                pos = token.pos_\n",
        "                pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
        "\n",
        "            total_tokens = len(doc)\n",
        "\n",
        "            # Calculate ratios\n",
        "            noun_ratio = pos_counts.get('NOUN', 0) / total_tokens if total_tokens > 0 else 0\n",
        "            verb_ratio = pos_counts.get('VERB', 0) / total_tokens if total_tokens > 0 else 0\n",
        "            adj_ratio = pos_counts.get('ADJ', 0) / total_tokens if total_tokens > 0 else 0\n",
        "            adv_ratio = pos_counts.get('ADV', 0) / total_tokens if total_tokens > 0 else 0\n",
        "            prop_noun_ratio = pos_counts.get('PROPN', 0) / total_tokens if total_tokens > 0 else 0\n",
        "            num_ratio = pos_counts.get('NUM', 0) / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "\n",
        "\n",
        "            legal_term_count = sum(1 for token in doc if token.text.lower() in legal_terms)\n",
        "\n",
        "            doc_pos_features.append({\n",
        "                'noun_ratio': noun_ratio,\n",
        "                'verb_ratio': verb_ratio,\n",
        "                'adj_ratio': adj_ratio,\n",
        "                'adv_ratio': adv_ratio,\n",
        "                'prop_noun_ratio': prop_noun_ratio,\n",
        "                'num_ratio': num_ratio,\n",
        "                'legal_term_count': legal_term_count\n",
        "            })\n",
        "\n",
        "        all_pos_features.append(doc_pos_features)\n",
        "\n",
        "    return all_pos_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b71907-8e3f-4aee-bbfe-41bf965ae96b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "19b71907-8e3f-4aee-bbfe-41bf965ae96b"
      },
      "source": [
        "## Postion Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bc133208-063e-4ecb-98fc-4828d6244c33",
      "metadata": {
        "id": "bc133208-063e-4ecb-98fc-4828d6244c33"
      },
      "outputs": [],
      "source": [
        "# 3. Position features and sentence length\n",
        "def calculate_position_features(df):\n",
        "    print(\"Calculating position and length features...\")\n",
        "\n",
        "    all_position_features = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing documents\"):\n",
        "        source_sentences = row['sentences']\n",
        "        total_sentences = len(source_sentences)\n",
        "\n",
        "        position_features = []\n",
        "        for i, sentence in enumerate(source_sentences):\n",
        "            # Calculate position features\n",
        "            rel_position = i / total_sentences if total_sentences > 0 else 0\n",
        "            is_first_quarter = 1 if rel_position <= 0.25 else 0\n",
        "            is_last_quarter = 1 if rel_position >= 0.75 else 0\n",
        "\n",
        "            sent_length = len(sentence.split())\n",
        "\n",
        "            # POS ratios (would need to implement this separately with spaCy)\n",
        "\n",
        "            position_features.append({\n",
        "                'rel_position': rel_position,\n",
        "                'is_first_quarter': is_first_quarter,\n",
        "                'is_last_quarter': is_last_quarter,\n",
        "                'is_first': 1 if i == 0 else 0,\n",
        "                'is_last': 1 if i == total_sentences - 1 else 0,\n",
        "                'sentence_length': sent_length,\n",
        "                'sentence_parity': i % 2  # 0 for even, 1 for odd positions\n",
        "            })\n",
        "\n",
        "        all_position_features.append(position_features)\n",
        "\n",
        "    return all_position_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85a78268-8553-4adc-affb-bac2d867da75",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "85a78268-8553-4adc-affb-bac2d867da75"
      },
      "source": [
        "## ROUGE Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "84b5c059-6813-4dfe-b298-b1b3e58b4a55",
      "metadata": {
        "id": "84b5c059-6813-4dfe-b298-b1b3e58b4a55"
      },
      "outputs": [],
      "source": [
        "# 2. ROUGE-based labeling\n",
        "def calculate_rouge_scores(df):\n",
        "    print(\"Calculating ROUGE scores for each sentence...\")\n",
        "    rouge = Rouge()\n",
        "\n",
        "    all_sentence_scores = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing documents\"):\n",
        "        source_sentences = row['sentences']\n",
        "        reference_summary = row['summary/long']\n",
        "\n",
        "        sentence_scores = []\n",
        "        for sentence in source_sentences:\n",
        "            try:\n",
        "                # Calculate ROUGE scores between this sentence and the reference summary\n",
        "                scores = rouge.get_scores(sentence, reference_summary)[0]\n",
        "\n",
        "                # Extract metrics\n",
        "                rouge1_f = scores['rouge-1']['f']\n",
        "                rouge2_f = scores['rouge-2']['f']\n",
        "                rougeL_f = scores['rouge-l']['f']\n",
        "\n",
        "                # Save all scores\n",
        "                sentence_scores.append({\n",
        "                    'rouge1_f': rouge1_f,\n",
        "                    'rouge2_f': rouge2_f,\n",
        "                    'rougeL_f': rougeL_f\n",
        "                })\n",
        "            except Exception as e:\n",
        "                # Handle empty sentences\n",
        "                sentence_scores.append({\n",
        "                    'rouge1_f': 0.0,\n",
        "                    'rouge2_f': 0.0,\n",
        "                    'rougeL_f': 0.0\n",
        "                })\n",
        "\n",
        "        all_sentence_scores.append(sentence_scores)\n",
        "\n",
        "    return all_sentence_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a2c05a-87aa-427a-a7c1-1f83ce63606f",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "02a2c05a-87aa-427a-a7c1-1f83ce63606f"
      },
      "source": [
        "## TF-IDF & Cousin Similarity Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "71bd933b-0dd0-433b-9169-849df37c072e",
      "metadata": {
        "id": "71bd933b-0dd0-433b-9169-849df37c072e"
      },
      "outputs": [],
      "source": [
        "def fit_and_save_tfidf(df, save_path=\"tfidf_data.pkl\", max_features=5000):\n",
        "    print(\"Fitting TF-IDF on corpus...\")\n",
        "\n",
        "    # Flatten all sentences\n",
        "    all_sentences = [s for sentence_list in df['sentences_tf_idf'] for s in sentence_list]\n",
        "\n",
        "    # Fit vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(all_sentences)\n",
        "\n",
        "    # Save both vectorizer and matrix\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            \"vectorizer\": tfidf_vectorizer,\n",
        "            \"matrix\": tfidf_matrix,\n",
        "            \"sentences\": all_sentences  # Needed to map matrix rows back\n",
        "        }, f)\n",
        "\n",
        "\n",
        "    print(f\"TF-IDF vectorizer and matrix saved to {save_path}\")\n",
        "\n",
        "    return tfidf_vectorizer, tfidf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "64999fce-5da1-4b63-ac75-683172b3e08f",
      "metadata": {
        "id": "64999fce-5da1-4b63-ac75-683172b3e08f"
      },
      "outputs": [],
      "source": [
        "def transform_and_save_tfidf(df, tfidf_vectorizer, save_path=\"tfidf_data.pkl\", max_features=5000):\n",
        "    print(\"Fitting TF-IDF on corpus...\")\n",
        "\n",
        "    # Flatten all sentences\n",
        "    all_sentences = [s for sentence_list in df['sentences_tf_idf'] for s in sentence_list]\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(all_sentences)\n",
        "\n",
        "    # Save both vectorizer and matrix\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            \"vectorizer\": tfidf_vectorizer,\n",
        "            \"matrix\": tfidf_matrix,\n",
        "            \"sentences\": all_sentences  # Needed to map matrix rows back\n",
        "        }, f)\n",
        "\n",
        "\n",
        "    print(f\"TF-IDF vectorizer and matrix saved to {save_path}\")\n",
        "    return tfidf_vectorizer, tfidf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f2a1bcf6-6b21-4314-91da-ff25d31d66ca",
      "metadata": {
        "id": "f2a1bcf6-6b21-4314-91da-ff25d31d66ca"
      },
      "outputs": [],
      "source": [
        "def calculate_tfidf_features(df, tfidf_matrix):\n",
        "    \"\"\"\n",
        "    Calculates sentence-level features using a precomputed TF-IDF matrix:\n",
        "    - avg_tfidf\n",
        "    - max_tfidf\n",
        "    - sum_tfidf\n",
        "    - centroid_similarity (cosine similarity to the document centroid)\n",
        "    \"\"\"\n",
        "    print(\"Calculating TF-IDF features using precomputed matrix...\")\n",
        "\n",
        "    sentence_features = []\n",
        "    sentence_idx = 0\n",
        "\n",
        "    for sentence_list in tqdm(df['sentences_tf_idf'], desc=\"Processing documents\"):\n",
        "        doc_features = []\n",
        "        doc_vectors = []\n",
        "\n",
        "        # Collect TF-IDF vectors for all sentences in the current document\n",
        "        for _ in sentence_list:\n",
        "            vec = tfidf_matrix[sentence_idx].toarray()[0]\n",
        "            doc_vectors.append(vec)\n",
        "            sentence_idx += 1\n",
        "\n",
        "        doc_vectors_np = np.array(doc_vectors)\n",
        "        centroid = np.mean(doc_vectors_np, axis=0).reshape(1, -1)\n",
        "        centroid_similarities = cosine_similarity(doc_vectors_np, centroid).flatten()\n",
        "\n",
        "        # Create features for each sentence\n",
        "        for i, vec in enumerate(doc_vectors):\n",
        "            doc_features.append({\n",
        "                'avg_tfidf': np.mean(vec),\n",
        "                'max_tfidf': np.max(vec),\n",
        "                'sum_tfidf': np.sum(vec),\n",
        "                'centroid_similarity': centroid_similarities[i]\n",
        "            })\n",
        "\n",
        "        sentence_features.append(doc_features)\n",
        "\n",
        "    return sentence_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb19e11-1f16-429c-b289-7a68c27c0342",
      "metadata": {
        "id": "8cb19e11-1f16-429c-b289-7a68c27c0342"
      },
      "source": [
        "## Combine & save Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ae0382-39ae-4cd5-b313-44261d21fdc4",
      "metadata": {
        "id": "06ae0382-39ae-4cd5-b313-44261d21fdc4"
      },
      "source": [
        "### Generate Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf05bf78-06cc-4454-b807-3b0192843b61",
      "metadata": {
        "id": "bf05bf78-06cc-4454-b807-3b0192843b61"
      },
      "source": [
        "LexNlp requires Python v3.6, so we did run this function on another environement, we will just import it for now as a json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b746b751-c337-48d5-a7d4-8a24efc9b5dc",
      "metadata": {
        "id": "b746b751-c337-48d5-a7d4-8a24efc9b5dc"
      },
      "outputs": [],
      "source": [
        "# ner_features = calculate_entity_features(df)\n",
        "\n",
        "with open('ner_features.json', 'r') as file:\n",
        "    ner_features = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1700a3f5-71e2-44df-a7d5-03fd7bd32ee5",
      "metadata": {
        "id": "1700a3f5-71e2-44df-a7d5-03fd7bd32ee5"
      },
      "outputs": [],
      "source": [
        "rouge_features = calculate_rouge_scores(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe65a4b-106d-4c5c-9f58-2c94cbdcb107",
      "metadata": {
        "id": "8fe65a4b-106d-4c5c-9f58-2c94cbdcb107"
      },
      "outputs": [],
      "source": [
        "position_features = calculate_position_features(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5d11ca-f365-4281-9ec3-41ecc8ed6575",
      "metadata": {
        "id": "dc5d11ca-f365-4281-9ec3-41ecc8ed6575"
      },
      "outputs": [],
      "source": [
        "pos_features = calculate_pos_features(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6490b0e5-0bd0-4f1a-80c8-71c631a3cfa8",
      "metadata": {
        "id": "6490b0e5-0bd0-4f1a-80c8-71c631a3cfa8"
      },
      "source": [
        "split data for tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce4d3f19-a855-4ed5-98a9-793da038041c",
      "metadata": {
        "id": "ce4d3f19-a855-4ed5-98a9-793da038041c"
      },
      "outputs": [],
      "source": [
        "split_idx = int(len(df) * 0.8)\n",
        "X_train = df.iloc[:split_idx]\n",
        "X_test = df.iloc[split_idx:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a25edded-b68d-434d-b5f4-cf0bad99e684",
      "metadata": {
        "id": "a25edded-b68d-434d-b5f4-cf0bad99e684"
      },
      "source": [
        "tfidf for train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cebdc3d-7e86-410c-b05c-5c94a43e70cb",
      "metadata": {
        "id": "9cebdc3d-7e86-410c-b05c-5c94a43e70cb"
      },
      "outputs": [],
      "source": [
        "vectorizer, tfidf_matrix = fit_and_save_tfidf(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4871a3-4f2f-4717-aeac-6e1fa23b15e7",
      "metadata": {
        "id": "1e4871a3-4f2f-4717-aeac-6e1fa23b15e7"
      },
      "outputs": [],
      "source": [
        "tfidf_train = calculate_tfidf_features(X_train, tfidf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d1452ad-9afe-4669-86b8-f18f8bec407c",
      "metadata": {
        "id": "9d1452ad-9afe-4669-86b8-f18f8bec407c"
      },
      "source": [
        "tfidf for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68103523-a46b-4e75-9426-1259e3654ec6",
      "metadata": {
        "id": "68103523-a46b-4e75-9426-1259e3654ec6"
      },
      "outputs": [],
      "source": [
        "vectorizer, tfidf_matrix_test = transform_and_save_tfidf(X_test, vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02c5245-4a3e-42cd-acef-a4dec8317af2",
      "metadata": {
        "id": "d02c5245-4a3e-42cd-acef-a4dec8317af2"
      },
      "outputs": [],
      "source": [
        "tfidf_test = calculate_tfidf_features(X_test, tfidf_matrix_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e28da98d-dae4-45a5-8b6f-95efd661f1bd",
      "metadata": {
        "id": "e28da98d-dae4-45a5-8b6f-95efd661f1bd"
      },
      "source": [
        "combine tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edef80b-660c-4bed-898f-d28b485f0237",
      "metadata": {
        "id": "1edef80b-660c-4bed-898f-d28b485f0237"
      },
      "outputs": [],
      "source": [
        "tfidf_features = tfidf_train + tfidf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0814c0-4361-4c06-9c1a-843056bdf7c5",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "7e0814c0-4361-4c06-9c1a-843056bdf7c5"
      },
      "source": [
        "### Build Features Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ed1d313-44f6-4a2e-a3bb-95739fa1b22a",
      "metadata": {
        "id": "4ed1d313-44f6-4a2e-a3bb-95739fa1b22a"
      },
      "outputs": [],
      "source": [
        "def build_enhanced_feature_matrix_dynamic(df,\n",
        "                                          tfidf_features,\n",
        "                                          rouge_scores,\n",
        "                                          position_features,\n",
        "                                          pos_features,\n",
        "                                          ner_features,\n",
        "                                          top_pct=0.2):\n",
        "    \"\"\"\n",
        "    Build feature matrices & dynamic labels where the top `top_pct` fraction of\n",
        "    sentences (by rougeL_f) in each document are labeled 1, the rest 0.\n",
        "    \"\"\"\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    for doc_idx in range(len(df)):\n",
        "        doc_feats = []\n",
        "        doc_lbls = []\n",
        "\n",
        "        # per-doc arrays\n",
        "        doc_tfidf     = tfidf_features[doc_idx]\n",
        "        doc_rouge     = rouge_scores[doc_idx]\n",
        "        doc_position  = position_features[doc_idx]\n",
        "        doc_pos       = pos_features[doc_idx]\n",
        "        doc_ner = ner_features[doc_idx]\n",
        "        num_sentences = len(doc_rouge)\n",
        "\n",
        "        # Determine how many to pick\n",
        "        k = max(1, ceil(top_pct * num_sentences))\n",
        "\n",
        "        # Get indices sorted by rougeL_f descending\n",
        "        sorted_idxs = sorted(\n",
        "            range(num_sentences),\n",
        "            key=lambda i: doc_rouge[i]['rougeL_f'],\n",
        "            reverse=True\n",
        "        )\n",
        "        topk_idxs = set(sorted_idxs[:k])\n",
        "\n",
        "        for i in range(num_sentences):\n",
        "            # optional: skip very short sentences\n",
        "            if i < len(doc_position) and doc_position[i]['sentence_length'] <= 2:\n",
        "                continue\n",
        "\n",
        "            # bounds check\n",
        "            if i >= len(doc_tfidf) or i >= len(doc_position) or i >= len(doc_pos):\n",
        "                continue\n",
        "\n",
        "            # dynamic label\n",
        "            label = 1 if i in topk_idxs else 0\n",
        "\n",
        "            # assemble features\n",
        "            feats = {\n",
        "                **doc_tfidf[i],\n",
        "                **doc_rouge[i],\n",
        "                **doc_position[i],\n",
        "                **doc_pos[i],\n",
        "                **doc_ner[i]\n",
        "            }\n",
        "\n",
        "            doc_feats.append(feats)\n",
        "            doc_lbls.append(label)\n",
        "\n",
        "        all_features.append(doc_feats)\n",
        "        all_labels.append(doc_lbls)\n",
        "\n",
        "    return all_features, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4f4321-eacc-4723-964b-a514fe18c928",
      "metadata": {
        "id": "2a4f4321-eacc-4723-964b-a514fe18c928"
      },
      "outputs": [],
      "source": [
        "def remove_rouge_features(all_features):\n",
        "    keys_to_remove = ['rouge1_f', 'rouge2_f', 'rougeL_f']\n",
        "    cleaned = []\n",
        "\n",
        "    for doc in all_features:\n",
        "        cleaned_doc = []\n",
        "        for sentence in doc:\n",
        "            filtered = {k: v for k, v in sentence.items() if k not in keys_to_remove}\n",
        "            cleaned_doc.append(filtered)\n",
        "        cleaned.append(cleaned_doc)\n",
        "\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ae1710-b679-4807-b499-2970724997d5",
      "metadata": {
        "id": "d0ae1710-b679-4807-b499-2970724997d5"
      },
      "outputs": [],
      "source": [
        "all_features, all_labels = build_enhanced_feature_matrix_dynamic(df,tfidf_features, rouge_features, position_features, pos_features, ner_features)\n",
        "all_features_cleaned = remove_rouge_features(all_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "865b1887-9cf9-4bbf-bac6-b9aa12f6a022",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "865b1887-9cf9-4bbf-bac6-b9aa12f6a022"
      },
      "source": [
        "### Save Combined Features as JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "022f3d44-9366-4c45-bfd8-cd55fa34a0df",
      "metadata": {
        "id": "022f3d44-9366-4c45-bfd8-cd55fa34a0df"
      },
      "outputs": [],
      "source": [
        "all_features_combined = {\n",
        "    \"all_features\": all_features_cleaned,\n",
        "    \"all_labels\" : all_labels\n",
        "}\n",
        "\n",
        "\n",
        "# Save to file\n",
        "with open(\"all_features_combined.json\", \"w\") as f:\n",
        "    json.dump(all_features_comined, f)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dc8db0ec-c7fa-40a6-beb5-8748e682dbdc",
        "711be60c-5c47-4641-b394-74219e114caa",
        "8969f1dd-0f9e-427b-99c6-409153c397a5",
        "911920c2-4f88-462b-a249-3b60973a437d",
        "YRvkdQjkrxyg",
        "2911da1c-e84a-4d5d-bd21-ed445e9bc49c",
        "6965aa01-e357-49d5-9b7d-d8e0a882f77a",
        "19b71907-8e3f-4aee-bbfe-41bf965ae96b",
        "85a78268-8553-4adc-affb-bac2d867da75",
        "02a2c05a-87aa-427a-a7c1-1f83ce63606f",
        "06ae0382-39ae-4cd5-b313-44261d21fdc4",
        "7e0814c0-4361-4c06-9c1a-843056bdf7c5"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}